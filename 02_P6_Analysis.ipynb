{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_P6_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxu6WXJUL7fQkBqlSvZi63",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adil1979/P6_CentraleSupelec_OCR/blob/master/02_P6_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSop45eqebf6"
      },
      "source": [
        "# **Projet6: Catégorisation automatique de questions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d2JN0EmexRG"
      },
      "source": [
        "## **1.Librairies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQeDyOh9e59r"
      },
      "source": [
        "!pip install feature_engine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-68rY2afKEY"
      },
      "source": [
        "!pip install plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qef7GyJftkd"
      },
      "source": [
        "!pip install chart-studio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSKusbMofUTn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib as mpl\n",
        "import chart_studio.plotly.plotly as py\n",
        "#import plotly.plotly as py\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "import calendar as cld\n",
        "from scipy import stats\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import scale\n",
        "from os import listdir\n",
        "import glob\n",
        "import missingno as msno\n",
        "from feature_engine.categorical_encoders import OneHotCategoricalEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import dates\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "import string\n",
        "from collections import Counter\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrTQifPy6BIA"
      },
      "source": [
        "SELECT Id, Body, Title, Tags\n",
        "\n",
        "FROM Posts\n",
        "\n",
        "WHERE Score >= 3 AND PostTypeId = 1\n",
        "\n",
        "ORDER BY RAND()\n",
        " \n",
        "ASC OFFSET 0 ROWS FETCH NEXT 50000 ROWS ONLY; de 0 à 50000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VMJmTqtf8gC"
      },
      "source": [
        "## **2.Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkIlB69CfiLQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItEOW0AnhhI-"
      },
      "source": [
        "df_analysis_01 = pd.read_csv('/content/drive/My Drive/CentraleSupelec_OCR/P6/QueryResults_01.csv')\n",
        "df_analysis_01.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grs-8H-KjSoc"
      },
      "source": [
        "df_analysis_01.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIt_V1W_iYuI"
      },
      "source": [
        "df_analysis_02 = pd.read_csv('/content/drive/My Drive/CentraleSupelec_OCR/P6/QueryResults_02.csv')\n",
        "df_analysis_02.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efABEHKwjhBM"
      },
      "source": [
        "df_analysis_02.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pr79O4riiS8"
      },
      "source": [
        "df_analysis_03 = pd.read_csv('/content/drive/My Drive/CentraleSupelec_OCR/P6/QueryResults_03.csv')\n",
        "df_analysis_03.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pguxJor9iuvx"
      },
      "source": [
        "df_analysis_03.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfDab1r0jqvL"
      },
      "source": [
        "frames = [df_analysis_01, df_analysis_02, df_analysis_03]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uENy71CmPr7"
      },
      "source": [
        "df_analysis = pd.concat(frames)\n",
        "df_analysis.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjC0-DdB9Ugl"
      },
      "source": [
        "df_sample = df_analysis.sample(n=40000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5XpRFZ5mg9n"
      },
      "source": [
        "df_sample.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsRWYtvyoBZW"
      },
      "source": [
        "df_sample = df_sample.loc[:, ['Id', 'Title', 'Body', 'Tags']]\n",
        "df_sample = df_sample.set_index('Id')\n",
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsq5NNeQY1MM"
      },
      "source": [
        "## **3.Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSS6pmwSuWSe"
      },
      "source": [
        "df_sample.duplicated(subset=[\"Body\", \"Title\"]).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4NtewDAK46S"
      },
      "source": [
        "example_body = df_sample.Body.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0FcnRlmLaop"
      },
      "source": [
        "example_body"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-o4p8QYLgz9"
      },
      "source": [
        "example_title = df_sample.Title.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EJVGUYKLsGM"
      },
      "source": [
        "example_title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9luITnRArx8"
      },
      "source": [
        "#Merge Title and Body\n",
        "df_sample['Title_Body'] = df_sample['Title'] + ' ' + df_sample['Body']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOEE0BMDvNJr"
      },
      "source": [
        "df_sample.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otKOZCqRvbF-"
      },
      "source": [
        "variables_reorganization = ['Title_Body', 'Tags']\n",
        "df_sample = df_sample[['Title_Body', 'Tags']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW2klWNWv9Ca"
      },
      "source": [
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41veqd4nsmUH"
      },
      "source": [
        "df_sample.Title_Body.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODJZRQTKtawy"
      },
      "source": [
        "#HTML characters removing\n",
        "def remove_html(body):\n",
        "  html_regex = re.compile('<.*?>') #Compile regular expresions\n",
        "  return re.sub(html_regex, ' ', str(body)) # Replace regex by ' '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuOICImzpMJe"
      },
      "source": [
        "df_sample['Title_Body'] = df_sample['Title_Body'].apply(remove_html)\n",
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkoq83Wch7_0"
      },
      "source": [
        "df_sample.Title_Body.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5QvAxQnwylj"
      },
      "source": [
        "#URL removing\n",
        "def remove_url(body):\n",
        "  url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "  return re.sub(url_regex, ' ', str(body))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVZuq5NeL6E-"
      },
      "source": [
        "df_sample['Title_Body'] = df_sample['Title_Body'].apply(remove_url)\n",
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L43cenkZiH6I"
      },
      "source": [
        "df_sample.Title_Body.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg1XwcS7qzcX"
      },
      "source": [
        "#Punctuation removing\n",
        "def remove_punc(body):\n",
        "  clean_text = re.sub(r'[?|!|\"|:|=|_|{|}|[|]|-|$|%|^|&|]',r' ',str(body))\n",
        "  clean_text = re.sub(r'[.|,|)|(|\\|/|-|~|`|>|<|*|$|@|;|→]',r' ', clean_text)\n",
        "  return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbLLFOAZ9x9k"
      },
      "source": [
        "df_sample['Title_Body'] = df_sample['Title_Body'].apply(remove_punc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1v00_8giS9s"
      },
      "source": [
        "df_sample.Title_Body.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhnRPj4ye-L7"
      },
      "source": [
        "#All characters removing except letters, numbers \n",
        "#Lower\n",
        "def remove_other(body):\n",
        "  text = str(body)\n",
        "  clean_text = re.sub(r\"[^a-zA-Z0-9#+-]\", \" \", text.lower())\n",
        "  return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ODVUIyvgSsq"
      },
      "source": [
        "df_sample['Title_Body'] = df_sample['Title_Body'].apply(remove_other)\n",
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ICRcIezhPF0"
      },
      "source": [
        "df_sample.Title_Body.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIdM-GNzyse6"
      },
      "source": [
        "#Space removing\n",
        "def remove_space(body):\n",
        "  return ' '.join(str(body).split()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjvChyO17tes"
      },
      "source": [
        "df_sample['Title_Body'] = df_sample['Title_Body'].apply(remove_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZa6Pe2i8A4S"
      },
      "source": [
        "df_sample.Title_Body.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NipGuoPUgc80"
      },
      "source": [
        "#df_analysis['Title_Body_count'] = df_analysis['Title_Body'].apply(lambda text: len(text.split(\" \")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nl2g1o8geKB"
      },
      "source": [
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os74ax1iEZ2S"
      },
      "source": [
        "#Tokenization\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXZRI_NEUX0t"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer().lemmatize\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoP_cdg6JOug"
      },
      "source": [
        "def remove_encoding_text(text):\n",
        "    text = str(text)\n",
        "    text = \" \".join(word for word in text.split() if word not in stop_words)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHi-5wlWBpj"
      },
      "source": [
        "df_sample['Title_Body'] = df_sample['Title_Body'].apply(remove_encoding_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaSzDJUSo3xm"
      },
      "source": [
        "df_sample.Title_Body.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YaLawhYUw8s"
      },
      "source": [
        "df_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-V0t_yeVciA"
      },
      "source": [
        "def tokenize(document):\n",
        "  tokens = [lemma(w) for w in document.split() if w.isalpha()]\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUhbCOxqYPP6"
      },
      "source": [
        "df_sample['Title_Body'].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0N-PFG0BYTy"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcg5tQc0ElRd"
      },
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer = tokenize, stop_words = stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVOwBLVRYg2g"
      },
      "source": [
        "tfidf = vectorizer.fit_transform(df_sample['Title_Body'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_ECctYlQaNk",
        "outputId": "86ff022a-b346-4af7-a05f-ffef17d06581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "df_sample.Title_Body.iloc[0]"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'asp net core web api class library trying make n tier application web api kept different class library made testcontroller controller class different class library code goes like using microsoft aspnetcore mvc using system using system collections generic namespace pkruni sms api apicontroller produces application json route api controller public class testcontroller controllerbase public ienumerable lt string gt get return new string chris hadfield question access web api main project already added reference class library main project working api working 404 page found error shows trying access api project structure wrong please help'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK4FRNXsaeBb"
      },
      "source": [
        "vectorizer.vocabulary_.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsYPzxHIcJ7i"
      },
      "source": [
        "#get word frequencies and create wordcloud\n",
        "tfidf_weights = [(word, tfidf.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS_KpDp_pxYp"
      },
      "source": [
        "type(tfidf_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f53oTZZp4ML"
      },
      "source": [
        "tfidf_weights[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVDlAPkvlk2B"
      },
      "source": [
        "from wordcloud import WordCloud"
      ],
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW3h_Q1Cp7RR"
      },
      "source": [
        "w = WordCloud(width=1500, height=1200, mode='RGBA', background_color='white', max_words=2000).fit_words(dict(tfidf_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OezkUp0Prcc_"
      },
      "source": [
        "# all below in a single cell\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.imshow(w)\n",
        "plt.axis('off')\n",
        "plt.savefig('recipes_wordcloud.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah_xE9GDoAE9"
      },
      "source": [
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUhCTBNB-no3"
      },
      "source": [
        "df_analysis.Tags.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y78r0JExSVE"
      },
      "source": [
        "# convert data to list\n",
        "to_list = {'><': \"','\",'<':\"['\", '>':\"']\"}\n",
        "for key, value in to_list.items():\n",
        "  df_analysis['Tags'] = df_analysis['Tags'].str.replace(key, value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scRJmW0gJbV1"
      },
      "source": [
        "df_analysis.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmZwCDae3Leh"
      },
      "source": [
        "tags = df_analysis['Tags'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--59jO6D3PyO"
      },
      "source": [
        "tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD87WUq6viug"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer # Convert a collection of text documents to a matrix of token counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvfihWT0vjrK"
      },
      "source": [
        "cv = CountVectorizer(max_features = 50) #, analyzer= 'word', stop_words= 'english')\n",
        "tags_vec = cv.fit_transform(tags).toarray()\n",
        "print('Number of tags: ', len(cv.vocabulary_))\n",
        "tags_name = cv.get_feature_names()\n",
        "tags_count = tags_vec.sum(axis=0)\n",
        "print('count of Tags: ')\n",
        "print(dict(zip(tags_name, tags_count)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgJgWuYqF0tU"
      },
      "source": [
        "dic_tags = dict(zip(tags_name, tags_count))\n",
        "dic_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvfjKiusqXdK"
      },
      "source": [
        "#plt.bar(list(dic_tags.keys()), dic_tags.values(), color='g', width= 0.5)\n",
        "#plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIox0LfOYeS9"
      },
      "source": [
        "df_tags_count = pd.DataFrame(dic_tags.items(), columns = ['Tag', 'Count']).sort_values(by='Count', ascending = False )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VO_XAiQdOXA"
      },
      "source": [
        "df_tags_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arDsG2gVdRLI"
      },
      "source": [
        "#df_sorted = df_tags_count.sort_values(by='Count', ascending=False)\n",
        "\n",
        "sns.catplot(x='Tag', y='Count', data=df_tags_count, kind='bar', height=15, aspect=10.7/1.27);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9arIP1XyvdLN"
      },
      "source": [
        "ax  = sns.countplot(x = 'Tag', data = df_tags_count.head(10))\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SIp5pfKvB0N"
      },
      "source": [
        "Depuis le départ, on a seulement utilisé les fréquences d'apparition des différents mots/n-grammes présents dans notre corpus. Le problème est que si l'on veut vraiment représenter un document par les n-grammes qu'il contient, il faudrait le faire relativement à leur apparition dans les autres documents.\n",
        "\n",
        "En effet, si un mot apparait dans d'autres documents, il est donc moins représentatif du document qu'un mot qui n'apparait que uniquement dans ce document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4UoCPqa3drV"
      },
      "source": [
        "I will be covering these topics in the project, but you can go through them before or during the project.\n",
        "\n",
        "1. http://www.tfidf.com/ - Gives the formula for TF-IDF calculation and a simple example.\n",
        "\n",
        "2. Difference between stemming and lemmatizing -\n",
        "\n",
        "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdC9BOp6_6HN"
      },
      "source": [
        "## **TF-IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSQIwcTsDLZ3"
      },
      "source": [
        "Depuis le départ, on a seulement utilisé les fréquences d'apparition des différents mots/n-grammes présents dans notre corpus. Le problème est que si l'on veut vraiment représenter un document par les n-grammes qu'il contient, il faudrait le faire relativement à leur apparition dans les autres documents.\n",
        "\n",
        "En effet, si un mot apparait dans d'autres documents, il est donc moins représentatif du document qu'un mot qui n'apparait que uniquement dans ce document.\n",
        "\n",
        "I will be covering these topics in the project, but you can go through them before or during the project.\n",
        "\n",
        "http://www.tfidf.com/ - Gives the formula for TF-IDF calculation and a simple example.\n",
        "\n",
        "Difference between stemming and lemmatizing -\n",
        "\n",
        "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma."
      ]
    }
  ]
}